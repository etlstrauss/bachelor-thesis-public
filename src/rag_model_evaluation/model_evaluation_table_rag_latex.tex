\footnote{The questions are replaced by the ID given in the questions overview. The responses are not shown, due to formatting reasons. They are accessible in the GitHub repository.} 
\begin{longtable}{|c|p{7cm}|c|c|}
\toprule
id & model & question & review \\
\midrule
1 & qwen2.5-coder:32b-instruct-q8\_0 & 1 & 2 \\
2 & qwen2.5-coder:32b-instruct-q8\_0 & 2 & 2 \\
3 & qwen2.5-coder:32b-instruct-q8\_0 & 3 & 2 \\
4 & qwen2.5-coder:32b-instruct-q8\_0 & 4 & 2 \\
5 & qwen2.5-coder:32b-instruct-q8\_0 & 5 & 2 \\
6 & qwen2.5-coder:32b-instruct-q8\_0 & 6 & 2 \\
7 & qwen2.5-coder:32b-instruct-q8\_0 & 7 & 2 \\
8 & qwen2.5-coder:32b-instruct-q8\_0 & 8 & 2 \\
9 & qwen2.5-coder:32b-instruct-q8\_0 & 9 & 2 \\
10 & qwen2.5-coder:32b-instruct-q8\_0 & 10 & 2 \\
11 & qwen2.5-coder:32b-instruct-q8\_0 & 11 & 2 \\
12 & qwen2.5-coder:32b-instruct-q8\_0 & 12 & 1 \\
13 & qwen2.5-coder:32b-instruct-q8\_0 & 13 & 1 \\
14 & qwen2.5-coder:32b-instruct-q8\_0 & 14 & 1 \\
15 & qwen2.5-coder:32b-instruct-q8\_0 & 15 & 2 \\
16 & mistral-small:22b-instruct-2409-fp16 & 1 & 2 \\
17 & mistral-small:22b-instruct-2409-fp16 & 2 & 2 \\
18 & mistral-small:22b-instruct-2409-fp16 & 3 & 2 \\
19 & mistral-small:22b-instruct-2409-fp16 & 4 & 0 \\
20 & mistral-small:22b-instruct-2409-fp16 & 5 & 1 \\
21 & mistral-small:22b-instruct-2409-fp16 & 6 & 1 \\
22 & mistral-small:22b-instruct-2409-fp16 & 7 & 1 \\
23 & mistral-small:22b-instruct-2409-fp16 & 8 & 1 \\
24 & mistral-small:22b-instruct-2409-fp16 & 9 & 1 \\
25 & mistral-small:22b-instruct-2409-fp16 & 10 & 2 \\
26 & mistral-small:22b-instruct-2409-fp16 & 11 & 1 \\
27 & mistral-small:22b-instruct-2409-fp16 & 12 & 2 \\
28 & mistral-small:22b-instruct-2409-fp16 & 13 & 1 \\
29 & mistral-small:22b-instruct-2409-fp16 & 14 & 0 \\
30 & mistral-small:22b-instruct-2409-fp16 & 15 & 1 \\
31 & llama3.1:70b & 1 & 2 \\
32 & llama3.1:70b & 2 & 2 \\
33 & llama3.1:70b & 3 & 2 \\
34 & llama3.1:70b & 4 & 0 \\
35 & llama3.1:70b & 5 & 1 \\
36 & llama3.1:70b & 6 & 1 \\
37 & llama3.1:70b & 7 & 2 \\
38 & llama3.1:70b & 8 & 1 \\
39 & llama3.1:70b & 9 & 2 \\
40 & llama3.1:70b & 10 & 1 \\
41 & llama3.1:70b & 11 & 2 \\
42 & llama3.1:70b & 12 & 2 \\
43 & llama3.1:70b & 13 & 1 \\
44 & llama3.1:70b & 14 & 1 \\
45 & llama3.1:70b & 15 & 2 \\
46 & llama3.3:70b-instruct-q4\_K\_M & 1 & 2 \\
47 & llama3.3:70b-instruct-q4\_K\_M & 2 & 1 \\
48 & llama3.3:70b-instruct-q4\_K\_M & 3 & 1 \\
49 & llama3.3:70b-instruct-q4\_K\_M & 4 & 0 \\
50 & llama3.3:70b-instruct-q4\_K\_M & 5 & 2 \\
51 & llama3.3:70b-instruct-q4\_K\_M & 6 & 2 \\
52 & llama3.3:70b-instruct-q4\_K\_M & 7 & 2 \\
53 & llama3.3:70b-instruct-q4\_K\_M & 8 & 1 \\
54 & llama3.3:70b-instruct-q4\_K\_M & 9 & 0 \\
55 & llama3.3:70b-instruct-q4\_K\_M & 10 & 0 \\
56 & llama3.3:70b-instruct-q4\_K\_M & 11 & 1 \\
57 & llama3.3:70b-instruct-q4\_K\_M & 12 & 1 \\
58 & llama3.3:70b-instruct-q4\_K\_M & 13 & 2 \\
59 & llama3.3:70b-instruct-q4\_K\_M & 14 & 2 \\
60 & llama3.3:70b-instruct-q4\_K\_M & 15 & 2 \\
61 & hf.co/matteogeniaccio/phi-4:F16 & 1 & 2 \\
62 & hf.co/matteogeniaccio/phi-4:F16 & 2 & 2 \\
63 & hf.co/matteogeniaccio/phi-4:F16 & 3 & 2 \\
64 & hf.co/matteogeniaccio/phi-4:F16 & 4 & 1 \\
65 & hf.co/matteogeniaccio/phi-4:F16 & 5 & 1 \\
66 & hf.co/matteogeniaccio/phi-4:F16 & 6 & 1 \\
67 & hf.co/matteogeniaccio/phi-4:F16 & 7 & 1 \\
68 & hf.co/matteogeniaccio/phi-4:F16 & 8 & 2 \\
69 & hf.co/matteogeniaccio/phi-4:F16 & 9 & 2 \\
70 & hf.co/matteogeniaccio/phi-4:F16 & 10 & 2 \\
71 & hf.co/matteogeniaccio/phi-4:F16 & 11 & 0 \\
72 & hf.co/matteogeniaccio/phi-4:F16 & 12 & 1 \\
73 & hf.co/matteogeniaccio/phi-4:F16 & 13 & 2 \\
74 & hf.co/matteogeniaccio/phi-4:F16 & 14 & 0 \\
75 & hf.co/matteogeniaccio/phi-4:F16 & 15 & 2 \\
\bottomrule
\caption[]{Table of Rating of FlowiseAI RAG Implementation Calls}
\end{longtable}
