{
  "nodes": [
    {
      "id": "seqLLMNode_0",
      "position": {
        "x": 731.5787536215304,
        "y": 277.53445322115226
      },
      "type": "customNode",
      "data": {
        "id": "seqLLMNode_0",
        "label": "LLM Node",
        "version": 3,
        "name": "seqLLMNode",
        "type": "LLMNode",
        "baseClasses": [
          "LLMNode"
        ],
        "category": "Sequential Agents",
        "description": "Run Chat Model and return the output",
        "inputParams": [
          {
            "label": "Name",
            "name": "llmNodeName",
            "type": "string",
            "placeholder": "LLM",
            "id": "seqLLMNode_0-input-llmNodeName-string"
          },
          {
            "label": "System Prompt",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_0-input-systemMessagePrompt-string"
          },
          {
            "label": "Human Prompt",
            "name": "humanMessagePrompt",
            "type": "string",
            "description": "This prompt will be added at the end of the messages as human message",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_0-input-humanMessagePrompt-string"
          },
          {
            "label": "Messages History",
            "name": "messageHistory",
            "description": "Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples",
            "type": "code",
            "hideCodeExecute": true,
            "codeExample": "const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\n\nreturn [\n    new HumanMessage(\"What is 333382 ðŸ¦œ 1932?\"),\n    new AIMessage({\n        content: \"\",\n        tool_calls: [\n        {\n            id: \"12345\",\n            name: \"calulator\",\n            args: {\n                number1: 333382,\n                number2: 1932,\n                operation: \"divide\",\n            },\n        },\n        ],\n    }),\n    new ToolMessage({\n        tool_call_id: \"12345\",\n        content: \"The answer is 172.558.\",\n    }),\n    new AIMessage(\"The answer is 172.558.\"),\n]",
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_0-input-messageHistory-code"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "description": "Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "additionalParams": true,
            "id": "seqLLMNode_0-input-promptValues-json"
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "type": "datagrid",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "datagrid": [
              {
                "field": "key",
                "headerName": "Key",
                "editable": true
              },
              {
                "field": "type",
                "headerName": "Type",
                "type": "singleSelect",
                "valueOptions": [
                  "String",
                  "String Array",
                  "Number",
                  "Boolean",
                  "Enum"
                ],
                "editable": true
              },
              {
                "field": "enumValues",
                "headerName": "Enum Values",
                "editable": true
              },
              {
                "field": "description",
                "headerName": "Description",
                "flex": 1,
                "editable": true
              }
            ],
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_0-input-llmStructuredOutput-datagrid"
          },
          {
            "label": "Update State",
            "name": "updateStateMemory",
            "type": "tabs",
            "tabIdentifier": "selectedUpdateStateMemoryTab",
            "default": "updateStateMemoryUI",
            "additionalParams": true,
            "tabs": [
              {
                "label": "Update State (Table)",
                "name": "updateStateMemoryUI",
                "type": "datagrid",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Key and value pair to be updated. For example: if you have the following State:\n    | Key       | Operation     | Default Value     |\n    |-----------|---------------|-------------------|\n    | user      | Replace       |                   |\n\n    You can update the \"user\" value with the following:\n    | Key       | Value     |\n    |-----------|-----------|\n    | user      | john doe  |\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\n    | Key       | Value                     |\n    |-----------|---------------------------|\n    | user      | `$flow.output.content`  |\n\n3. You can get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values",
                "datagrid": [
                  {
                    "field": "key",
                    "headerName": "Key",
                    "type": "asyncSingleSelect",
                    "loadMethod": "loadStateKeys",
                    "flex": 0.5,
                    "editable": true
                  },
                  {
                    "field": "value",
                    "headerName": "Value",
                    "type": "freeSolo",
                    "valueOptions": [
                      {
                        "label": "LLM Node Output (string)",
                        "value": "$flow.output.content"
                      },
                      {
                        "label": "LLM JSON Output Key (string)",
                        "value": "$flow.output.<replace-with-key>"
                      },
                      {
                        "label": "Global variable (string)",
                        "value": "$vars.<variable-name>"
                      },
                      {
                        "label": "Input Question (string)",
                        "value": "$flow.input"
                      },
                      {
                        "label": "Session Id (string)",
                        "value": "$flow.sessionId"
                      },
                      {
                        "label": "Chat Id (string)",
                        "value": "$flow.chatId"
                      },
                      {
                        "label": "Chatflow Id (string)",
                        "value": "$flow.chatflowId"
                      }
                    ],
                    "editable": true,
                    "flex": 1
                  }
                ],
                "optional": true,
                "additionalParams": true
              },
              {
                "label": "Update State (Code)",
                "name": "updateStateMemoryCode",
                "type": "code",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Return the key value JSON object. For example: if you have the following State:\n    ```json\n    {\n        \"user\": null\n    }\n    ```\n\n    You can update the \"user\" value by returning the following:\n    ```js\n    return {\n        \"user\": \"john doe\"\n    }\n    ```\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\n    ```js\n    return {\n        \"user\": $flow.output.content\n    }\n    ```\n\n3. You can also get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state",
                "hideCodeExecute": true,
                "codeExample": "const result = $flow.output;\n\n/* Suppose we have a custom State schema like this:\n* {\n    aggregate: {\n        value: (x, y) => x.concat(y),\n        default: () => []\n    }\n  }\n*/\n\nreturn {\n  aggregate: [result.content]\n};",
                "optional": true,
                "additionalParams": true
              }
            ],
            "id": "seqLLMNode_0-input-updateStateMemory-tabs"
          }
        ],
        "inputAnchors": [
          {
            "label": "Start | Agent | Condition | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Start | Agent | Condition | LLMNode | ToolNode",
            "list": true,
            "id": "seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
          },
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Overwrite model to be used for this node",
            "id": "seqLLMNode_0-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "llmNodeName": "Creatot",
          "systemMessagePrompt": "You should write python files. Please only give the clear python code. Format the code correctly, but please markdown formats. ONLY CODE! No annotations or explanaitons! And ONLY ONE PYTHON FILE - NOTHING ELSE",
          "humanMessagePrompt": "Users Tasks:\n{task}",
          "messageHistory": "",
          "sequentialNode": [
            "{{seqStart_0.data.instance}}"
          ],
          "model": "",
          "promptValues": "{\"task\":\"{{question}}\"}",
          "llmStructuredOutput": "",
          "updateStateMemory": "updateStateMemoryUI"
        },
        "outputAnchors": [
          {
            "id": "seqLLMNode_0-output-seqLLMNode-LLMNode",
            "name": "seqLLMNode",
            "label": "LLMNode",
            "description": "Run Chat Model and return the output",
            "type": "LLMNode"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 451,
      "selected": false,
      "positionAbsolute": {
        "x": 731.5787536215304,
        "y": 277.53445322115226
      },
      "dragging": false
    },
    {
      "id": "seqStart_0",
      "position": {
        "x": 306.2223160794652,
        "y": 220.3313461034262
      },
      "type": "customNode",
      "data": {
        "id": "seqStart_0",
        "label": "Start",
        "version": 2,
        "name": "seqStart",
        "type": "Start",
        "baseClasses": [
          "Start"
        ],
        "category": "Sequential Agents",
        "description": "Starting point of the conversation",
        "inputParams": [],
        "inputAnchors": [
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "description": "Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat",
            "id": "seqStart_0-input-model-BaseChatModel"
          },
          {
            "label": "Agent Memory",
            "name": "agentMemory",
            "type": "BaseCheckpointSaver",
            "description": "Save the state of the agent",
            "optional": true,
            "id": "seqStart_0-input-agentMemory-BaseCheckpointSaver"
          },
          {
            "label": "State",
            "name": "state",
            "type": "State",
            "description": "State is an object that is updated by nodes in the graph, passing from one node to another. By default, state contains \"messages\" that got updated with each message sent and received.",
            "optional": true,
            "id": "seqStart_0-input-state-State"
          },
          {
            "label": "Input Moderation",
            "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
            "name": "inputModeration",
            "type": "Moderation",
            "optional": true,
            "list": true,
            "id": "seqStart_0-input-inputModeration-Moderation"
          }
        ],
        "inputs": {
          "model": "{{chatOllama_0.data.instance}}",
          "agentMemory": "",
          "state": "",
          "inputModeration": ""
        },
        "outputAnchors": [
          {
            "id": "seqStart_0-output-seqStart-Start",
            "name": "seqStart",
            "label": "Start",
            "description": "Starting point of the conversation",
            "type": "Start"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 383,
      "selected": false,
      "positionAbsolute": {
        "x": 306.2223160794652,
        "y": 220.3313461034262
      },
      "dragging": false
    },
    {
      "id": "chatOllama_0",
      "position": {
        "x": -127.78697697358987,
        "y": 72.88643367327484
      },
      "type": "customNode",
      "data": {
        "id": "chatOllama_0",
        "label": "ChatOllama",
        "version": 3,
        "name": "chatOllama",
        "type": "ChatOllama",
        "baseClasses": [
          "ChatOllama",
          "BaseChatModel",
          "BaseLanguageModel",
          "Runnable"
        ],
        "category": "Chat Models",
        "description": "Chat completion using open-source LLM on Ollama",
        "inputParams": [
          {
            "label": "Base URL",
            "name": "baseUrl",
            "type": "string",
            "default": "http://localhost:11434",
            "id": "chatOllama_0-input-baseUrl-string"
          },
          {
            "label": "Model Name",
            "name": "modelName",
            "type": "string",
            "placeholder": "llama2",
            "id": "chatOllama_0-input-modelName-string"
          },
          {
            "label": "Temperature",
            "name": "temperature",
            "type": "number",
            "description": "The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "default": 0.9,
            "optional": true,
            "id": "chatOllama_0-input-temperature-number"
          },
          {
            "label": "Keep Alive",
            "name": "keepAlive",
            "type": "string",
            "description": "How long to keep connection alive. A duration string (such as \"10m\" or \"24h\")",
            "default": "5m",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-keepAlive-string"
          },
          {
            "label": "Top P",
            "name": "topP",
            "type": "number",
            "description": "Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topP-number"
          },
          {
            "label": "Top K",
            "name": "topK",
            "type": "number",
            "description": "Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-topK-number"
          },
          {
            "label": "Mirostat",
            "name": "mirostat",
            "type": "number",
            "description": "Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostat-number"
          },
          {
            "label": "Mirostat ETA",
            "name": "mirostatEta",
            "type": "number",
            "description": "Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatEta-number"
          },
          {
            "label": "Mirostat TAU",
            "name": "mirostatTau",
            "type": "number",
            "description": "Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-mirostatTau-number"
          },
          {
            "label": "Context Window Size",
            "name": "numCtx",
            "type": "number",
            "description": "Sets the size of the context window used to generate the next token. (Default: 2048) Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numCtx-number"
          },
          {
            "label": "Number of GPU",
            "name": "numGpu",
            "type": "number",
            "description": "The number of layers to send to the GPU(s). On macOS it defaults to 1 to enable metal support, 0 to disable. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numGpu-number"
          },
          {
            "label": "Number of Thread",
            "name": "numThread",
            "type": "number",
            "description": "Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-numThread-number"
          },
          {
            "label": "Repeat Last N",
            "name": "repeatLastN",
            "type": "number",
            "description": "Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatLastN-number"
          },
          {
            "label": "Repeat Penalty",
            "name": "repeatPenalty",
            "type": "number",
            "description": "Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-repeatPenalty-number"
          },
          {
            "label": "Stop Sequence",
            "name": "stop",
            "type": "string",
            "rows": 4,
            "placeholder": "AI assistant:",
            "description": "Sets the stop sequences to use. Use comma to seperate different sequences. Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-stop-string"
          },
          {
            "label": "Tail Free Sampling",
            "name": "tfsZ",
            "type": "number",
            "description": "Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (Default: 1). Refer to <a target=\"_blank\" href=\"https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\">docs</a> for more details",
            "step": 0.1,
            "optional": true,
            "additionalParams": true,
            "id": "chatOllama_0-input-tfsZ-number"
          }
        ],
        "inputAnchors": [
          {
            "label": "Cache",
            "name": "cache",
            "type": "BaseCache",
            "optional": true,
            "id": "chatOllama_0-input-cache-BaseCache"
          }
        ],
        "inputs": {
          "cache": "",
          "baseUrl": "http://localhost:11434",
          "modelName": "qwen2.5-coder:32b-instruct-q8_0",
          "temperature": "0.3",
          "keepAlive": "5m",
          "topP": "",
          "topK": "",
          "mirostat": "",
          "mirostatEta": "",
          "mirostatTau": "",
          "numCtx": "",
          "numGpu": "",
          "numThread": "",
          "repeatLastN": "",
          "repeatPenalty": "",
          "stop": "",
          "tfsZ": ""
        },
        "outputAnchors": [
          {
            "id": "chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
            "name": "chatOllama",
            "label": "ChatOllama",
            "description": "Chat completion using open-source LLM on Ollama",
            "type": "ChatOllama | BaseChatModel | BaseLanguageModel | Runnable"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 580,
      "selected": false,
      "positionAbsolute": {
        "x": -127.78697697358987,
        "y": 72.88643367327484
      },
      "dragging": false
    },
    {
      "id": "seqEnd_0",
      "position": {
        "x": 2192.9447874143098,
        "y": 375.6378593645682
      },
      "type": "customNode",
      "data": {
        "id": "seqEnd_0",
        "label": "End",
        "version": 2,
        "name": "seqEnd",
        "type": "End",
        "baseClasses": [
          "End"
        ],
        "category": "Sequential Agents",
        "description": "End conversation",
        "inputParams": [],
        "inputAnchors": [
          {
            "label": "Agent | Condition | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Agent | Condition | LLMNode | ToolNode",
            "id": "seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode"
          }
        ],
        "inputs": {
          "sequentialNode": "{{seqLLMNode_1.data.instance}}"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 143,
      "selected": false,
      "positionAbsolute": {
        "x": 2192.9447874143098,
        "y": 375.6378593645682
      },
      "dragging": false
    },
    {
      "id": "seqConditionAgent_0",
      "position": {
        "x": 1144.0554808793042,
        "y": 188.9088410133076
      },
      "type": "customNode",
      "data": {
        "id": "seqConditionAgent_0",
        "label": "Condition Agent",
        "version": 2,
        "name": "seqConditionAgent",
        "type": "ConditionAgent",
        "baseClasses": [
          "ConditionAgent"
        ],
        "category": "Sequential Agents",
        "description": "Uses an agent to determine which route to take next",
        "inputParams": [
          {
            "label": "Name",
            "name": "conditionAgentName",
            "type": "string",
            "placeholder": "Condition Agent",
            "id": "seqConditionAgent_0-input-conditionAgentName-string"
          },
          {
            "label": "System Prompt",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "default": "You are an expert customer support routing system.\nYour job is to detect whether a customer support representative is routing a user to the technical support team, or just responding conversationally.",
            "additionalParams": true,
            "optional": true,
            "id": "seqConditionAgent_0-input-systemMessagePrompt-string"
          },
          {
            "label": "Human Prompt",
            "name": "humanMessagePrompt",
            "type": "string",
            "description": "This prompt will be added at the end of the messages as human message",
            "rows": 4,
            "default": "The previous conversation is an interaction between a customer support representative and a user.\nExtract whether the representative is routing the user to the technical support team, or just responding conversationally.\n\nIf representative want to route the user to the technical support team, respond only with the word \"TECHNICAL\".\nOtherwise, respond only with the word \"CONVERSATION\".\n\nRemember, only respond with one of the above words.",
            "additionalParams": true,
            "optional": true,
            "id": "seqConditionAgent_0-input-humanMessagePrompt-string"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "description": "Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "additionalParams": true,
            "id": "seqConditionAgent_0-input-promptValues-json"
          },
          {
            "label": "JSON Structured Output",
            "name": "conditionAgentStructuredOutput",
            "type": "datagrid",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "datagrid": [
              {
                "field": "key",
                "headerName": "Key",
                "editable": true
              },
              {
                "field": "type",
                "headerName": "Type",
                "type": "singleSelect",
                "valueOptions": [
                  "String",
                  "String Array",
                  "Number",
                  "Boolean",
                  "Enum"
                ],
                "editable": true
              },
              {
                "field": "enumValues",
                "headerName": "Enum Values",
                "editable": true
              },
              {
                "field": "description",
                "headerName": "Description",
                "flex": 1,
                "editable": true
              }
            ],
            "optional": true,
            "additionalParams": true,
            "id": "seqConditionAgent_0-input-conditionAgentStructuredOutput-datagrid"
          },
          {
            "label": "Condition",
            "name": "condition",
            "type": "conditionFunction",
            "tabIdentifier": "selectedConditionFunctionTab",
            "tabs": [
              {
                "label": "Condition (Table)",
                "name": "conditionUI",
                "type": "datagrid",
                "description": "If a condition is met, the node connected to the respective output will be executed",
                "optional": true,
                "datagrid": [
                  {
                    "field": "variable",
                    "headerName": "Variable",
                    "type": "freeSolo",
                    "editable": true,
                    "loadMethod": [
                      "getPreviousMessages",
                      "loadStateKeys"
                    ],
                    "valueOptions": [
                      {
                        "label": "Agent Output (string)",
                        "value": "$flow.output.content"
                      },
                      {
                        "label": "Agent's JSON Key Output (string)",
                        "value": "$flow.output.<replace-with-key>"
                      },
                      {
                        "label": "Total Messages (number)",
                        "value": "$flow.state.messages.length"
                      },
                      {
                        "label": "First Message Content (string)",
                        "value": "$flow.state.messages[0].content"
                      },
                      {
                        "label": "Last Message Content (string)",
                        "value": "$flow.state.messages[-1].content"
                      },
                      {
                        "label": "Global variable (string)",
                        "value": "$vars.<variable-name>"
                      }
                    ],
                    "flex": 0.5,
                    "minWidth": 200
                  },
                  {
                    "field": "operation",
                    "headerName": "Operation",
                    "type": "singleSelect",
                    "valueOptions": [
                      "Contains",
                      "Not Contains",
                      "Start With",
                      "End With",
                      "Is",
                      "Is Not",
                      "Is Empty",
                      "Is Not Empty",
                      "Greater Than",
                      "Less Than",
                      "Equal To",
                      "Not Equal To",
                      "Greater Than or Equal To",
                      "Less Than or Equal To"
                    ],
                    "editable": true,
                    "flex": 0.4,
                    "minWidth": 150
                  },
                  {
                    "field": "value",
                    "headerName": "Value",
                    "flex": 1,
                    "editable": true
                  },
                  {
                    "field": "output",
                    "headerName": "Output Name",
                    "editable": true,
                    "flex": 0.3,
                    "minWidth": 150
                  }
                ]
              },
              {
                "label": "Condition (Code)",
                "name": "conditionFunction",
                "type": "code",
                "description": "Function to evaluate the condition",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Must return a string value at the end of function. For example:\n    ```js\n    if (\"X\" === \"X\") {\n        return \"Agent\"; // connect to next agent node\n    } else {\n        return \"End\"; // connect to end node\n    }\n    ```\n\n2. In most cases, you would probably get the last message to do some comparison. You can get all current messages from the state: `$flow.state.messages`:\n    ```json\n    [\n        {\n            \"content\": \"Hello! How can I assist you today?\",\n            \"name\": \"\",\n            \"additional_kwargs\": {},\n            \"response_metadata\": {},\n            \"tool_calls\": [],\n            \"invalid_tool_calls\": [],\n            \"usage_metadata\": {}\n        }\n    ]\n    ```\n\n    For example, to get the last message content:\n    ```js\n    const messages = $flow.state.messages;\n    const lastMessage = messages[messages.length - 1];\n\n    // Proceed to do something with the last message content\n    ```\n\n3. If you want to use the Condition Agent's output for conditional checks, it is available as `$flow.output` with the following structure:\n\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, we can check if the agent's output contains specific keyword:\n    ```js\n    const result = $flow.output.content;\n    \n    if (result.includes(\"some-keyword\")) {\n        return \"Agent\"; // connect to next agent node\n    } else {\n        return \"End\"; // connect to end node\n    }\n    ```\n\n    If Structured Output is enabled, `$flow.output` will be in the JSON format as defined in the Structured Output configuration:\n    ```json\n    {\n        \"foo\": 'var'\n    }\n    ```\n\n4. You can get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n5. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "hideCodeExecute": true,
                "codeExample": "const result = $flow.output.content;\n\nif (result.includes(\"some-keyword\")) {\n    return \"Agent\";\n}\n\nreturn \"End\";\n",
                "optional": true
              }
            ],
            "id": "seqConditionAgent_0-input-condition-conditionFunction"
          }
        ],
        "inputAnchors": [
          {
            "label": "Start | Agent | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Start | Agent | LLMNode | ToolNode",
            "list": true,
            "id": "seqConditionAgent_0-input-sequentialNode-Start | Agent | LLMNode | ToolNode"
          },
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Overwrite model to be used for this agent",
            "id": "seqConditionAgent_0-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "conditionAgentName": "condi",
          "sequentialNode": [
            "{{seqLLMNode_0.data.instance}}"
          ],
          "model": "",
          "systemMessagePrompt": "You should decide if the following code uses matplotlib.\nRetrun a JSON key, plt, with the values true or false (its a boolen). either the code uses matplolib = true or it contains not = false",
          "humanMessagePrompt": "Code:\n{code}",
          "promptValues": "{\"code\":\"{{seqLLMNode_0.data.instance}}\"}",
          "conditionAgentStructuredOutput": "[{\"key\":\"plt\",\"type\":\"Boolean\",\"enumValues\":\"\",\"description\":\"\",\"actions\":\"\",\"id\":0}]",
          "condition": "",
          "conditionFunction": "if ($flow.output.plt) {\n    return \"yes\";\n}\nelse{\n    return \"no\";\n}",
          "selectedConditionFunctionTab_seqConditionAgent_0": "conditionFunction"
        },
        "outputAnchors": [
          {
            "name": "output",
            "label": "Output",
            "type": "options",
            "options": [
              {
                "id": "seqConditionAgent_0-output-no-Condition",
                "name": "no",
                "label": "no",
                "type": "Condition",
                "isAnchor": true
              },
              {
                "id": "seqConditionAgent_0-output-yes-Condition",
                "name": "yes",
                "label": "yes",
                "type": "Condition",
                "isAnchor": true
              }
            ]
          }
        ],
        "outputs": {
          "output": "next"
        },
        "selected": false
      },
      "width": 300,
      "height": 579,
      "selected": false,
      "positionAbsolute": {
        "x": 1144.0554808793042,
        "y": 188.9088410133076
      },
      "dragging": false
    },
    {
      "id": "seqEnd_1",
      "position": {
        "x": 2159.2803993379753,
        "y": 918.3240209796459
      },
      "type": "customNode",
      "data": {
        "id": "seqEnd_1",
        "label": "End",
        "version": 2,
        "name": "seqEnd",
        "type": "End",
        "baseClasses": [
          "End"
        ],
        "category": "Sequential Agents",
        "description": "End conversation",
        "inputParams": [],
        "inputAnchors": [
          {
            "label": "Agent | Condition | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Agent | Condition | LLMNode | ToolNode",
            "id": "seqEnd_1-input-sequentialNode-Agent | Condition | LLMNode | ToolNode"
          }
        ],
        "inputs": {
          "sequentialNode": "{{seqLLMNode_2.data.instance}}"
        },
        "outputAnchors": [],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 143,
      "selected": false,
      "positionAbsolute": {
        "x": 2159.2803993379753,
        "y": 918.3240209796459
      },
      "dragging": false
    },
    {
      "id": "seqLLMNode_1",
      "position": {
        "x": 1632.8322718343884,
        "y": 234.0897216133132
      },
      "type": "customNode",
      "data": {
        "id": "seqLLMNode_1",
        "label": "LLM Node",
        "version": 3,
        "name": "seqLLMNode",
        "type": "LLMNode",
        "baseClasses": [
          "LLMNode"
        ],
        "category": "Sequential Agents",
        "description": "Run Chat Model and return the output",
        "inputParams": [
          {
            "label": "Name",
            "name": "llmNodeName",
            "type": "string",
            "placeholder": "LLM",
            "id": "seqLLMNode_1-input-llmNodeName-string"
          },
          {
            "label": "System Prompt",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_1-input-systemMessagePrompt-string"
          },
          {
            "label": "Human Prompt",
            "name": "humanMessagePrompt",
            "type": "string",
            "description": "This prompt will be added at the end of the messages as human message",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_1-input-humanMessagePrompt-string"
          },
          {
            "label": "Messages History",
            "name": "messageHistory",
            "description": "Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples",
            "type": "code",
            "hideCodeExecute": true,
            "codeExample": "const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\n\nreturn [\n    new HumanMessage(\"What is 333382 ðŸ¦œ 1932?\"),\n    new AIMessage({\n        content: \"\",\n        tool_calls: [\n        {\n            id: \"12345\",\n            name: \"calulator\",\n            args: {\n                number1: 333382,\n                number2: 1932,\n                operation: \"divide\",\n            },\n        },\n        ],\n    }),\n    new ToolMessage({\n        tool_call_id: \"12345\",\n        content: \"The answer is 172.558.\",\n    }),\n    new AIMessage(\"The answer is 172.558.\"),\n]",
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_1-input-messageHistory-code"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "description": "Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "additionalParams": true,
            "id": "seqLLMNode_1-input-promptValues-json"
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "type": "datagrid",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "datagrid": [
              {
                "field": "key",
                "headerName": "Key",
                "editable": true
              },
              {
                "field": "type",
                "headerName": "Type",
                "type": "singleSelect",
                "valueOptions": [
                  "String",
                  "String Array",
                  "Number",
                  "Boolean",
                  "Enum"
                ],
                "editable": true
              },
              {
                "field": "enumValues",
                "headerName": "Enum Values",
                "editable": true
              },
              {
                "field": "description",
                "headerName": "Description",
                "flex": 1,
                "editable": true
              }
            ],
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_1-input-llmStructuredOutput-datagrid"
          },
          {
            "label": "Update State",
            "name": "updateStateMemory",
            "type": "tabs",
            "tabIdentifier": "selectedUpdateStateMemoryTab",
            "default": "updateStateMemoryUI",
            "additionalParams": true,
            "tabs": [
              {
                "label": "Update State (Table)",
                "name": "updateStateMemoryUI",
                "type": "datagrid",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Key and value pair to be updated. For example: if you have the following State:\n    | Key       | Operation     | Default Value     |\n    |-----------|---------------|-------------------|\n    | user      | Replace       |                   |\n\n    You can update the \"user\" value with the following:\n    | Key       | Value     |\n    |-----------|-----------|\n    | user      | john doe  |\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\n    | Key       | Value                     |\n    |-----------|---------------------------|\n    | user      | `$flow.output.content`  |\n\n3. You can get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values",
                "datagrid": [
                  {
                    "field": "key",
                    "headerName": "Key",
                    "type": "asyncSingleSelect",
                    "loadMethod": "loadStateKeys",
                    "flex": 0.5,
                    "editable": true
                  },
                  {
                    "field": "value",
                    "headerName": "Value",
                    "type": "freeSolo",
                    "valueOptions": [
                      {
                        "label": "LLM Node Output (string)",
                        "value": "$flow.output.content"
                      },
                      {
                        "label": "LLM JSON Output Key (string)",
                        "value": "$flow.output.<replace-with-key>"
                      },
                      {
                        "label": "Global variable (string)",
                        "value": "$vars.<variable-name>"
                      },
                      {
                        "label": "Input Question (string)",
                        "value": "$flow.input"
                      },
                      {
                        "label": "Session Id (string)",
                        "value": "$flow.sessionId"
                      },
                      {
                        "label": "Chat Id (string)",
                        "value": "$flow.chatId"
                      },
                      {
                        "label": "Chatflow Id (string)",
                        "value": "$flow.chatflowId"
                      }
                    ],
                    "editable": true,
                    "flex": 1
                  }
                ],
                "optional": true,
                "additionalParams": true
              },
              {
                "label": "Update State (Code)",
                "name": "updateStateMemoryCode",
                "type": "code",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Return the key value JSON object. For example: if you have the following State:\n    ```json\n    {\n        \"user\": null\n    }\n    ```\n\n    You can update the \"user\" value by returning the following:\n    ```js\n    return {\n        \"user\": \"john doe\"\n    }\n    ```\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\n    ```js\n    return {\n        \"user\": $flow.output.content\n    }\n    ```\n\n3. You can also get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state",
                "hideCodeExecute": true,
                "codeExample": "const result = $flow.output;\n\n/* Suppose we have a custom State schema like this:\n* {\n    aggregate: {\n        value: (x, y) => x.concat(y),\n        default: () => []\n    }\n  }\n*/\n\nreturn {\n  aggregate: [result.content]\n};",
                "optional": true,
                "additionalParams": true
              }
            ],
            "id": "seqLLMNode_1-input-updateStateMemory-tabs"
          }
        ],
        "inputAnchors": [
          {
            "label": "Start | Agent | Condition | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Start | Agent | Condition | LLMNode | ToolNode",
            "list": true,
            "id": "seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
          },
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Overwrite model to be used for this node",
            "id": "seqLLMNode_1-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "llmNodeName": "no",
          "systemMessagePrompt": "JUST PASS THE FOLLOWING CODE. DON'T CHANGE IT. DO NO CALLS",
          "humanMessagePrompt": "{code}",
          "messageHistory": "",
          "sequentialNode": [
            "{{seqConditionAgent_0.data.instance}}"
          ],
          "model": "",
          "promptValues": "{\"code\":\"{{seqLLMNode_0.data.instance}}\"}",
          "llmStructuredOutput": "",
          "updateStateMemory": "updateStateMemoryUI"
        },
        "outputAnchors": [
          {
            "id": "seqLLMNode_1-output-seqLLMNode-LLMNode",
            "name": "seqLLMNode",
            "label": "LLMNode",
            "description": "Run Chat Model and return the output",
            "type": "LLMNode"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 451,
      "selected": false,
      "positionAbsolute": {
        "x": 1632.8322718343884,
        "y": 234.0897216133132
      },
      "dragging": false
    },
    {
      "id": "seqLLMNode_2",
      "position": {
        "x": 1655.616886606814,
        "y": 762.8086308002123
      },
      "type": "customNode",
      "data": {
        "id": "seqLLMNode_2",
        "label": "LLM Node",
        "version": 3,
        "name": "seqLLMNode",
        "type": "LLMNode",
        "baseClasses": [
          "LLMNode"
        ],
        "category": "Sequential Agents",
        "description": "Run Chat Model and return the output",
        "inputParams": [
          {
            "label": "Name",
            "name": "llmNodeName",
            "type": "string",
            "placeholder": "LLM",
            "id": "seqLLMNode_2-input-llmNodeName-string"
          },
          {
            "label": "System Prompt",
            "name": "systemMessagePrompt",
            "type": "string",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_2-input-systemMessagePrompt-string"
          },
          {
            "label": "Human Prompt",
            "name": "humanMessagePrompt",
            "type": "string",
            "description": "This prompt will be added at the end of the messages as human message",
            "rows": 4,
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_2-input-humanMessagePrompt-string"
          },
          {
            "label": "Messages History",
            "name": "messageHistory",
            "description": "Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples",
            "type": "code",
            "hideCodeExecute": true,
            "codeExample": "const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\n\nreturn [\n    new HumanMessage(\"What is 333382 ðŸ¦œ 1932?\"),\n    new AIMessage({\n        content: \"\",\n        tool_calls: [\n        {\n            id: \"12345\",\n            name: \"calulator\",\n            args: {\n                number1: 333382,\n                number2: 1932,\n                operation: \"divide\",\n            },\n        },\n        ],\n    }),\n    new ToolMessage({\n        tool_call_id: \"12345\",\n        content: \"The answer is 172.558.\",\n    }),\n    new AIMessage(\"The answer is 172.558.\"),\n]",
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_2-input-messageHistory-code"
          },
          {
            "label": "Format Prompt Values",
            "name": "promptValues",
            "description": "Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value",
            "type": "json",
            "optional": true,
            "acceptVariable": true,
            "list": true,
            "additionalParams": true,
            "id": "seqLLMNode_2-input-promptValues-json"
          },
          {
            "label": "JSON Structured Output",
            "name": "llmStructuredOutput",
            "type": "datagrid",
            "description": "Instruct the LLM to give output in a JSON structured schema",
            "datagrid": [
              {
                "field": "key",
                "headerName": "Key",
                "editable": true
              },
              {
                "field": "type",
                "headerName": "Type",
                "type": "singleSelect",
                "valueOptions": [
                  "String",
                  "String Array",
                  "Number",
                  "Boolean",
                  "Enum"
                ],
                "editable": true
              },
              {
                "field": "enumValues",
                "headerName": "Enum Values",
                "editable": true
              },
              {
                "field": "description",
                "headerName": "Description",
                "flex": 1,
                "editable": true
              }
            ],
            "optional": true,
            "additionalParams": true,
            "id": "seqLLMNode_2-input-llmStructuredOutput-datagrid"
          },
          {
            "label": "Update State",
            "name": "updateStateMemory",
            "type": "tabs",
            "tabIdentifier": "selectedUpdateStateMemoryTab",
            "default": "updateStateMemoryUI",
            "additionalParams": true,
            "tabs": [
              {
                "label": "Update State (Table)",
                "name": "updateStateMemoryUI",
                "type": "datagrid",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Key and value pair to be updated. For example: if you have the following State:\n    | Key       | Operation     | Default Value     |\n    |-----------|---------------|-------------------|\n    | user      | Replace       |                   |\n\n    You can update the \"user\" value with the following:\n    | Key       | Value     |\n    |-----------|-----------|\n    | user      | john doe  |\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\n    | Key       | Value                     |\n    |-----------|---------------------------|\n    | user      | `$flow.output.content`  |\n\n3. You can get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values",
                "datagrid": [
                  {
                    "field": "key",
                    "headerName": "Key",
                    "type": "asyncSingleSelect",
                    "loadMethod": "loadStateKeys",
                    "flex": 0.5,
                    "editable": true
                  },
                  {
                    "field": "value",
                    "headerName": "Value",
                    "type": "freeSolo",
                    "valueOptions": [
                      {
                        "label": "LLM Node Output (string)",
                        "value": "$flow.output.content"
                      },
                      {
                        "label": "LLM JSON Output Key (string)",
                        "value": "$flow.output.<replace-with-key>"
                      },
                      {
                        "label": "Global variable (string)",
                        "value": "$vars.<variable-name>"
                      },
                      {
                        "label": "Input Question (string)",
                        "value": "$flow.input"
                      },
                      {
                        "label": "Session Id (string)",
                        "value": "$flow.sessionId"
                      },
                      {
                        "label": "Chat Id (string)",
                        "value": "$flow.chatId"
                      },
                      {
                        "label": "Chatflow Id (string)",
                        "value": "$flow.chatflowId"
                      }
                    ],
                    "editable": true,
                    "flex": 1
                  }
                ],
                "optional": true,
                "additionalParams": true
              },
              {
                "label": "Update State (Code)",
                "name": "updateStateMemoryCode",
                "type": "code",
                "hint": {
                  "label": "How to use",
                  "value": "\n1. Return the key value JSON object. For example: if you have the following State:\n    ```json\n    {\n        \"user\": null\n    }\n    ```\n\n    You can update the \"user\" value by returning the following:\n    ```js\n    return {\n        \"user\": \"john doe\"\n    }\n    ```\n\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\n    ```json\n    {\n        \"content\": 'Hello! How can I assist you today?',\n        \"name\": \"\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {}\n    }\n    ```\n\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\n    ```js\n    return {\n        \"user\": $flow.output.content\n    }\n    ```\n\n3. You can also get default flow config, including the current \"state\":\n    - `$flow.sessionId`\n    - `$flow.chatId`\n    - `$flow.chatflowId`\n    - `$flow.input`\n    - `$flow.state`\n\n4. You can get custom variables: `$vars.<variable-name>`\n\n"
                },
                "description": "This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state",
                "hideCodeExecute": true,
                "codeExample": "const result = $flow.output;\n\n/* Suppose we have a custom State schema like this:\n* {\n    aggregate: {\n        value: (x, y) => x.concat(y),\n        default: () => []\n    }\n  }\n*/\n\nreturn {\n  aggregate: [result.content]\n};",
                "optional": true,
                "additionalParams": true
              }
            ],
            "id": "seqLLMNode_2-input-updateStateMemory-tabs"
          }
        ],
        "inputAnchors": [
          {
            "label": "Start | Agent | Condition | LLM | Tool Node",
            "name": "sequentialNode",
            "type": "Start | Agent | Condition | LLMNode | ToolNode",
            "list": true,
            "id": "seqLLMNode_2-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
          },
          {
            "label": "Chat Model",
            "name": "model",
            "type": "BaseChatModel",
            "optional": true,
            "description": "Overwrite model to be used for this node",
            "id": "seqLLMNode_2-input-model-BaseChatModel"
          }
        ],
        "inputs": {
          "llmNodeName": "yes",
          "systemMessagePrompt": "You will get some code to format. It is a python script, that contains matplotlib and espacially plt.show(). You should format it to markdown readeble. Therfore you have to save the figure into an buffer, format it into base64 and write it into an specific markdown format + output it. Here an example how that can look like:\n# write to buffer\nbuffer = io.BytesIO()\nplt.savefig(buffer, format='png', bbox_inches='tight')\nbuffer.seek(0)\n# Encode the image as Base64\nimg_base64 = base64.b64encode(buffer.read()).decode('utf-8')\nbuffer.close()\n# make it markdown readeble\nmarkdown_image = f\"![Figure](data:image/png;base64,{img_base64})\"\nprint(markdown_image)  # Output Markdown for embedding",
          "humanMessagePrompt": "This is the actually code:\n{code}\n\nIMPORTANT: JUST GIVE THE PYTHON CODE. I WANT NO EXPLANATIONS!",
          "messageHistory": "",
          "sequentialNode": [
            "{{seqConditionAgent_0.data.instance}}"
          ],
          "model": "",
          "promptValues": "{\"img_base64\":\"\",\"code\":\"{{seqLLMNode_0.data.instance}}\"}",
          "llmStructuredOutput": "",
          "updateStateMemory": "updateStateMemoryUI"
        },
        "outputAnchors": [
          {
            "id": "seqLLMNode_2-output-seqLLMNode-LLMNode",
            "name": "seqLLMNode",
            "label": "LLMNode",
            "description": "Run Chat Model and return the output",
            "type": "LLMNode"
          }
        ],
        "outputs": {},
        "selected": false
      },
      "width": 300,
      "height": 451,
      "selected": false,
      "positionAbsolute": {
        "x": 1655.616886606814,
        "y": 762.8086308002123
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "seqStart_0",
      "sourceHandle": "seqStart_0-output-seqStart-Start",
      "target": "seqLLMNode_0",
      "targetHandle": "seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqStart_0-seqStart_0-output-seqStart-Start-seqLLMNode_0-seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
    },
    {
      "source": "chatOllama_0",
      "sourceHandle": "chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable",
      "target": "seqStart_0",
      "targetHandle": "seqStart_0-input-model-BaseChatModel",
      "type": "buttonedge",
      "id": "chatOllama_0-chatOllama_0-output-chatOllama-ChatOllama|BaseChatModel|BaseLanguageModel|Runnable-seqStart_0-seqStart_0-input-model-BaseChatModel"
    },
    {
      "source": "seqLLMNode_0",
      "sourceHandle": "seqLLMNode_0-output-seqLLMNode-LLMNode",
      "target": "seqConditionAgent_0",
      "targetHandle": "seqConditionAgent_0-input-sequentialNode-Start | Agent | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqLLMNode_0-seqLLMNode_0-output-seqLLMNode-LLMNode-seqConditionAgent_0-seqConditionAgent_0-input-sequentialNode-Start | Agent | LLMNode | ToolNode"
    },
    {
      "source": "seqConditionAgent_0",
      "sourceHandle": "seqConditionAgent_0-output-no-Condition",
      "target": "seqLLMNode_1",
      "targetHandle": "seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqConditionAgent_0-seqConditionAgent_0-output-no-Condition-seqLLMNode_1-seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
    },
    {
      "source": "seqLLMNode_1",
      "sourceHandle": "seqLLMNode_1-output-seqLLMNode-LLMNode",
      "target": "seqEnd_0",
      "targetHandle": "seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqLLMNode_1-seqLLMNode_1-output-seqLLMNode-LLMNode-seqEnd_0-seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode"
    },
    {
      "source": "seqConditionAgent_0",
      "sourceHandle": "seqConditionAgent_0-output-yes-Condition",
      "target": "seqLLMNode_2",
      "targetHandle": "seqLLMNode_2-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqConditionAgent_0-seqConditionAgent_0-output-yes-Condition-seqLLMNode_2-seqLLMNode_2-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode"
    },
    {
      "source": "seqLLMNode_2",
      "sourceHandle": "seqLLMNode_2-output-seqLLMNode-LLMNode",
      "target": "seqEnd_1",
      "targetHandle": "seqEnd_1-input-sequentialNode-Agent | Condition | LLMNode | ToolNode",
      "type": "buttonedge",
      "id": "seqLLMNode_2-seqLLMNode_2-output-seqLLMNode-LLMNode-seqEnd_1-seqEnd_1-input-sequentialNode-Agent | Condition | LLMNode | ToolNode"
    }
  ]
}